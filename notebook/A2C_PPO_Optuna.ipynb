{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6f35ef",
   "metadata": {},
   "source": [
    "# A2C, PPO & Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bc9dc3",
   "metadata": {},
   "source": [
    "This notebook contains the code of the Optuna search, Advantage Actor Critic and Proximal Policy Optimization algorithms used in ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894a9339",
   "metadata": {},
   "source": [
    "## Imports and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35419c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import following packages\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Literal, Optional, Union, Any, Dict\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1f434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\n",
    "\n",
    "def tuple_or_list_to_str(list_tuple_list: list[Union[tuple, list]]) -> list[str]:\n",
    "\n",
    "    # Store the elements of a list with tuples/lists into a list with strings.\n",
    "\n",
    "    # Inputs\n",
    "\n",
    "    # list_tuple_list : list[Union[tuple, list]] List of tuples/lists to store into a list of strs.\n",
    "\n",
    "    # Outputs\n",
    "\n",
    "    # list[str]: Elements of the tuple/list stored in a str.\n",
    "\n",
    "    return list(map(lambda x: \" \".join(map(str, x)), list_tuple_list))\n",
    "\n",
    "\n",
    "# Environment class, needs to be a valid gym.Env class\n",
    "\n",
    "\n",
    "class HTC_env(gym.Env):\n",
    "\n",
    "    # Inputs\n",
    "\n",
    "    # size: Number of layers of the physical system, we use 16\n",
    "\n",
    "    # Initialization of the environment\n",
    "\n",
    "    def __init__(self, size):\n",
    "\n",
    "        super(HTC_env, self).__init__()  # Initialize parent classes\n",
    "\n",
    "        self.size = size  # Number of layers of the physical system\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            0, 1, shape=(self.size,), dtype=int\n",
    "        )  # Shape of the possible states\n",
    "        self.action_space = gym.spaces.Discrete(\n",
    "            2 * self.size\n",
    "        )  # Shape of the possibla actions\n",
    "\n",
    "        self.render_mode = (\n",
    "            None  # In case something asks about rendering the environment\n",
    "        )\n",
    "\n",
    "        self._counter = 0  # Countdown to reset the episode\n",
    "        self._max_counter = 2 * self.size  # Length of episode\n",
    "\n",
    "        self.terminated = False  # Checks if there is a need to reset the episode\n",
    "\n",
    "        self.seen = (\n",
    "            []\n",
    "        )  # Holds the states that have been visited by this environment instance\n",
    "        self.best = 0.0  # Holds the best state found of this environment instance\n",
    "        self.seen_recording = (\n",
    "            []\n",
    "        )  # Holds how many states have been seen as the environment is used\n",
    "        self.best_recording = (\n",
    "            []\n",
    "        )  # Holds which is the best state found yet, as the environment is used\n",
    "\n",
    "    # Internal function to calculate the next state and return given an action\n",
    "\n",
    "    def _take_action(self, action):\n",
    "\n",
    "        new_state_slice = np.copy(self._current_state[0])  # Copy the previous state\n",
    "\n",
    "        if action < self.size:\n",
    "\n",
    "            new_state_slice[action] = int(0)  # Make the selected layer dielctric\n",
    "\n",
    "        else:\n",
    "\n",
    "            new_state_slice[action - self.size] = int(\n",
    "                1\n",
    "            )  # Make the selected layer metallic\n",
    "\n",
    "        new_value = htc_series[\n",
    "            tuple_or_list_to_str([new_state_slice.astype(int)])\n",
    "        ].values[0] / (\n",
    "            norm\n",
    "        )  # HTC of the next state\n",
    "\n",
    "        new_state = [new_state_slice, new_value]  # New state tuple\n",
    "        reward = new_value.astype(\"float\") - rew_base  # Reward from new state\n",
    "\n",
    "        self._current_state = new_state  # Save this state as current\n",
    "\n",
    "        return new_state, reward\n",
    "\n",
    "    # Function to reset the state after the episode ends\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "\n",
    "        self.terminated = False  # Reset the flag\n",
    "\n",
    "        self._counter = 0  # Reset the countdown to reset the episode\n",
    "\n",
    "        super().reset(seed=seed)  # So that self.np_random is seeded, in case its needed\n",
    "\n",
    "        self._reset_state = np.zeros((self.size,)).astype(\n",
    "            int\n",
    "        )  # Seed for the starting state, all 0s\n",
    "\n",
    "        self._current_state = [\n",
    "            self._reset_state,\n",
    "            htc_series[tuple_or_list_to_str([self._reset_state])].values[0] / (norm),\n",
    "        ]  # Starting state, all 0s\n",
    "\n",
    "        state = self._current_state[0]  # Current state, back to 0\n",
    "        HTC = self._current_state[1]  # HTC of current state\n",
    "\n",
    "        return state, {}\n",
    "\n",
    "    # Function to apply an action and recover the next state and rewards\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        self._counter += 1  # Add one step to the counter\n",
    "\n",
    "        if (\n",
    "            self._counter == self._max_counter + 1\n",
    "        ):  # If next step would be outside episode, time to reset\n",
    "\n",
    "            next_state, _ = self.reset()  # Reset episode\n",
    "            reward = (\n",
    "                htc_series[tuple_or_list_to_str([next_state])].values[0] / (norm)\n",
    "                - rew_base\n",
    "            )  # What the reward would have been\n",
    "            self.terminated = True  # Put flag to true\n",
    "\n",
    "        else:\n",
    "\n",
    "            next_state, reward = self._take_action(\n",
    "                action\n",
    "            )  # Apply action to obtain new state and reward\n",
    "\n",
    "        if next_state[1] not in self.seen:  # Check if it's a new state\n",
    "\n",
    "            self.seen.append(next_state[1])  # Save the state to the list\n",
    "\n",
    "            if (\n",
    "                next_state[1] > self.best\n",
    "            ):  # If it's better than any before, save it as the best\n",
    "\n",
    "                self.best = next_state[1]\n",
    "\n",
    "            self.seen_recording.append(\n",
    "                len(self.seen)\n",
    "            )  # Save how many states we have seen so far\n",
    "            self.best_recording.append(self.best)  # Save which is the best among them\n",
    "\n",
    "        return (\n",
    "            next_state[0],\n",
    "            reward,\n",
    "            self.terminated,\n",
    "            False,\n",
    "            {},\n",
    "        )  # Output structure is the gym.Env structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2716b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining and loading the datasets\n",
    "\n",
    "vect = np.genfromtxt(\"../data/HTC/16layers_index.txt\").astype(\n",
    "    int\n",
    ")  # Combination of materials\n",
    "htc_vals = np.genfromtxt(\n",
    "    \"../data/HTC/16layer_data.txt\", dtype=\"float64\"\n",
    ")  # Associated HTC\n",
    "\n",
    "htc_series = pd.Series(\n",
    "    data=htc_vals, index=tuple_or_list_to_str(vect)\n",
    ")  # Series form, for much faster searching\n",
    "\n",
    "norm = 1e5  # Normalization constant for HTC, units (10^5 W/m^2 K)\n",
    "\n",
    "base_seed = np.array(\n",
    "    [1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1]\n",
    ")  # State to use as baseline for the rewards\n",
    "rew_base = htc_series[tuple_or_list_to_str([base_seed.astype(int)])][0] / (\n",
    "    norm\n",
    ")  # Baseline for the rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c0277",
   "metadata": {},
   "source": [
    "## Optuna search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb399d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna hyperparameter search, simple code\n",
    "\n",
    "# We choose A2C for the search, almost the same code is valid for PPO\n",
    "\n",
    "# Step 1: hyperparameters of the optuna search, random seeding and preallocations\n",
    "\n",
    "N_TRIALS = 100  # Maximum number of trials\n",
    "N_TIMESTEPS = int(1e5)  # Training budget\n",
    "N_EVAL_EPISODES = 32  # Length of each evaluation episode\n",
    "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
    "TIMEOUT = int(60 * 30 * 8)  # How long to wait for until stopping, in seconds\n",
    "\n",
    "seed = 42  # Random seed\n",
    "\n",
    "random.seed(seed)  # Seed the random library\n",
    "torch.manual_seed(seed)  # Seed the pytorch library\n",
    "np.random.seed(int(seed))  # Seed the numpy library\n",
    "\n",
    "# Dictionary that holds the hyperparameters that all trials will share and never explore\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": HTC_env(16),\n",
    "    \"use_rms_prop\": False,  # Delete in case of PPO\n",
    "}\n",
    "\n",
    "# Step 2: define external functions: evaluation of the model, sampling of hyperparams and the goal of the search\n",
    "\n",
    "# Function 1: evaluation for the performance of the model: stop and run 10 episodes, then average over them\n",
    "\n",
    "\n",
    "def custom_evaluation(env, model, len_eps):\n",
    "\n",
    "    # Will return the mean last reward of an episode\n",
    "\n",
    "    last_rewards = np.zeros((10,))\n",
    "\n",
    "    # Step 1: reset the environment\n",
    "\n",
    "    obs_evals, _ = env.reset()\n",
    "\n",
    "    # Step 2 (repeated 10 times): train the model for an episode, record the last reward\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        for j in range(len_eps):\n",
    "\n",
    "            action, _ = model.predict(obs_evals)\n",
    "            obs_evals, reward_evals, _, _, _ = env.step(action)\n",
    "\n",
    "        last_rewards[i] = reward_evals\n",
    "        obs_evals, _ = env.reset()\n",
    "\n",
    "    # Step 3: return the mean of the last rewards\n",
    "\n",
    "    return np.mean(last_rewards)\n",
    "\n",
    "\n",
    "# Function 2: parameter sampling for optuna\n",
    "\n",
    "\n",
    "def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "\n",
    "    # Hyperparameters we are searching for\n",
    "\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.5, 5.0, log=True)\n",
    "\n",
    "    gae_lambda = trial.suggest_float(\"gae_lambda\", 0.0, 1.0)\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "    # activation_fn = trial.suggest_categorical(\"activation_fn\", [\"selu\"])\n",
    "\n",
    "    # Hyperparameters we will leave fixed\n",
    "\n",
    "    activation_fn = nn.SELU\n",
    "    net_arch = {\"pi\": [100, 100, 100, 100], \"vf\": [100, 100, 100, 100]}\n",
    "    n_steps = 2**5\n",
    "\n",
    "    # Display true values\n",
    "\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    # Return dictionary with all chosen hyperparameters\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,  # Steps per episode\n",
    "        \"gamma\": gamma,  # Discount factor for RL\n",
    "        \"learning_rate\": learning_rate,  # Learning rate\n",
    "        \"max_grad_norm\": max_grad_norm,  # Maximum gradient\n",
    "        \"gae_lambda\": gae_lambda,  # GAE exponential factor\n",
    "        # \"batch_size\": n_steps,             # Size of the batches considered # Add in case of PPO\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,  # Network architecture (actor and critic)\n",
    "            \"activation_fn\": activation_fn,  # Activation function\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# Function 3: objective for optuna to search and evaluate configurations\n",
    "# Given a configuration, it will sample hyperparameters, evaluate it and report the result\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "    # 1. Sample hyperparameters and update the keyword arguments, adding the new ones to the default\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    kwargs.update(sample_a2c_params(trial))\n",
    "\n",
    "    # Create the RL model\n",
    "\n",
    "    model = A2C(**kwargs)\n",
    "\n",
    "    # 2. Create envs used for evaluation\n",
    "\n",
    "    eval_env = HTC_env(16)\n",
    "\n",
    "    # 3. Train the model\n",
    "\n",
    "    model.learn(N_TIMESTEPS)\n",
    "\n",
    "    # 4. Evaluate the trained model\n",
    "\n",
    "    score = custom_evaluation(eval_env, model, N_EVAL_EPISODES)\n",
    "\n",
    "    model.env.close()  # Reset environment\n",
    "    eval_env.close()  # Reset environment\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# Step 3: Perform the optuna search\n",
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "\n",
    "# Do not prune before 1/5 of the max budget is used\n",
    "\n",
    "pruner = MedianPruner(\n",
    "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_TIMESTEPS // 5\n",
    ")\n",
    "\n",
    "# Create the study and start the hyperparameter optimization\n",
    "\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=1, timeout=TIMEOUT)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "\n",
    "    pass\n",
    "\n",
    "# Step 4: print results\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial  # (.trials for all of them)\n",
    "\n",
    "print(f\"  Value: {trial.value}\")\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04bcc82",
   "metadata": {},
   "source": [
    "## A2C algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: pre-allocating, loading and previous steps\n",
    "\n",
    "seed = 42  # Fix the random seed\n",
    "\n",
    "random.seed(seed)  # Seed the random library\n",
    "torch.manual_seed(seed)  # Seed the pytorch library\n",
    "np.random.seed(int(seed))  # Seed the numpy library\n",
    "\n",
    "torch.backends.cudnn.deterministic = True  # Set cuda parameters\n",
    "torch.backends.cudnn.benchmark = False  # Set cuda parameters\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")  # Use GPU if available\n",
    "\n",
    "n_eval_episodes = 32  # Number of steps per evaluation episode\n",
    "N_timesteps = int(4000 * 32)  # Number of steps (4000 episodes of 32 steps each)\n",
    "\n",
    "torch.set_num_threads(1)  # Set pytorch num threads to 1 for faster training\n",
    "\n",
    "# Step 2: set the environment and the model hyperparameters\n",
    "\n",
    "our_env = HTC_env(16)  # Set environment (using the custom class)\n",
    "\n",
    "# Hyperparameters chosen by optuna\n",
    "\n",
    "n_steps = 2**5  # Steps per episode\n",
    "gamma = 1.0 - 0.05354748279639477  # Discount factor for RL\n",
    "learning_rate = 0.0003529867715614526  # Learning rate\n",
    "gae_lambda = 0.9972303247713417  # GAE exponential factor\n",
    "max_grad_norm = 1.2858503485554726  # Maximum gradient\n",
    "net_arch = {\n",
    "    \"pi\": [100, 100, 100, 100],\n",
    "    \"vf\": [100, 100, 100, 100],\n",
    "}  # Network architecture (actor and critic)\n",
    "activation_fn = nn.SELU  # Activation function\n",
    "\n",
    "# Hyperparameter dictionary\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": our_env,\n",
    "    \"use_rms_prop\": False,\n",
    "    \"device\": device,\n",
    "}\n",
    "\n",
    "Hyperparams = {\n",
    "    \"n_steps\": n_steps,\n",
    "    \"gamma\": gamma,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"max_grad_norm\": max_grad_norm,\n",
    "    \"gae_lambda\": gae_lambda,\n",
    "    \"policy_kwargs\": {\n",
    "        \"net_arch\": net_arch,\n",
    "        \"activation_fn\": activation_fn,\n",
    "    },\n",
    "}\n",
    "\n",
    "kwargs = DEFAULT_HYPERPARAMS.copy()  # Start the dictionary with the default ones\n",
    "kwargs.update(Hyperparams)  # Add the ones found by optuna\n",
    "\n",
    "# Step 3: train the model\n",
    "\n",
    "model = A2C(**kwargs)  # Create the model, using the previous hyperparams\n",
    "\n",
    "model.learn(N_timesteps)  # Train the model\n",
    "\n",
    "model.env.close()  # Close the env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806f098",
   "metadata": {},
   "source": [
    "## PPO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e240962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: pre-allocating, loading and previous steps\n",
    "\n",
    "seed = 42  # Fix the random seed\n",
    "\n",
    "random.seed(seed)  # Seed the random library\n",
    "torch.manual_seed(seed)  # Seed the pytorch library\n",
    "np.random.seed(int(seed))  # Seed the numpy library\n",
    "\n",
    "torch.backends.cudnn.deterministic = True  # Set cuda parameters\n",
    "torch.backends.cudnn.benchmark = False  # Set cuda parameters\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    ")  # Use GPU if available\n",
    "\n",
    "n_eval_episodes = 32  # Number of steps per evaluation episode\n",
    "N_timesteps = int(4000 * 32)  # Number of steps (4000 episodes of 32 steps each)\n",
    "\n",
    "torch.set_num_threads(1)  # Set pytorch num threads to 1 for faster training\n",
    "\n",
    "# Step 2: set the environment and the model hyperparameters\n",
    "\n",
    "our_env = HTC_env(16)  # Set environment (using the custom class)\n",
    "\n",
    "# Hyperparameters chosen by optuna\n",
    "\n",
    "n_steps = 2**5  # Steps per episode\n",
    "gamma = 1.0 - 0.00012343188973544484  # Discount factor for RL\n",
    "learning_rate = 6.24161237241945e-05  # Learning rate\n",
    "gae_lambda = 0.8714387482390391  # GAE exponential factor\n",
    "max_grad_norm = 4.042358248277151  # Maximum gradient\n",
    "net_arch = {\n",
    "    \"pi\": [100, 100, 100, 100],\n",
    "    \"vf\": [100, 100, 100, 100],\n",
    "}  # Network architecture (actor and critic)\n",
    "activation_fn = nn.SELU  # Activation function\n",
    "\n",
    "# Hyperparameter dictionary\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\"policy\": \"MlpPolicy\", \"env\": our_env, \"device\": device}\n",
    "\n",
    "Hyperparams = {\n",
    "    \"n_steps\": n_steps,\n",
    "    \"gamma\": gamma,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"max_grad_norm\": max_grad_norm,\n",
    "    \"gae_lambda\": gae_lambda,\n",
    "    \"batch_size\": n_steps,\n",
    "    \"policy_kwargs\": {\n",
    "        \"net_arch\": net_arch,\n",
    "        \"activation_fn\": activation_fn,\n",
    "    },\n",
    "}\n",
    "\n",
    "kwargs = DEFAULT_HYPERPARAMS.copy()  # Start the dictionary with the default ones\n",
    "kwargs.update(Hyperparams)  # Add the ones found by optuna\n",
    "\n",
    "# Step 3: train the model\n",
    "\n",
    "model = PPO(**kwargs)  # Create the model, using the previous hyperparams\n",
    "\n",
    "model.learn(N_timesteps)  # Train the model\n",
    "\n",
    "model.env.close()  # Close the env"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
