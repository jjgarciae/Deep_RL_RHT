{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bffac28",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aae232",
   "metadata": {},
   "source": [
    "This notebook contains the code of the REINFORCE algorithm used in ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d104bd4",
   "metadata": {},
   "source": [
    "## Imports and function definitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import following packages\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Literal, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90212da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions\n",
    "\n",
    "def tuple_or_list_to_str(list_tuple_list: list[Union[tuple, list]]) -> list[str]:\n",
    "    \n",
    "    # Store the elements of a list with tuples/lists into a list with strings.\n",
    "    \n",
    "    # Inputs\n",
    "\n",
    "    # list_tuple_list : list[Union[tuple, list]] List of tuples/lists to store into a list of strs.\n",
    "    \n",
    "    # Outputs\n",
    "    \n",
    "    # list[str]: Elements of the tuple/list stored in a str.\n",
    "    \n",
    "    return list(map(lambda x: \" \".join(map(str, x)), list_tuple_list))\n",
    "\n",
    "def act_on_env7(action, old_state, baseline):\n",
    "    \n",
    "    # Environment for the Reinforcement Learning Algorithm\n",
    "    \n",
    "    # Inputs\n",
    "    \n",
    "    # action: index describing the action to take: even index is a 0, odd ondex is a 1, in the floor(action/2) position\n",
    "    # old_state: vector of 0s and 1s describing the material\n",
    "    # baseline: HTC of the best system encountered so far, recorded in memory\n",
    "    \n",
    "    # Outputs\n",
    "    \n",
    "    # new_state: list of vector of 0s and 1s describing the material and the corresponding HTC\n",
    "    # reward: HTC of the new system, minus the baseline\n",
    "    \n",
    "    new_state_slice = np.copy(old_state[0]) # Copy the previous state\n",
    "    \n",
    "    if action < n_slice:\n",
    "        \n",
    "        new_state_slice[action] = int(0) # Make the selected layer dielctric\n",
    "        \n",
    "    else:\n",
    "            \n",
    "        new_state_slice[action-n_slice] = int(1) # Make the selected layer metallic\n",
    "    \n",
    "    new_value = htc_series[tuple_or_list_to_str([new_state_slice.astype(int)])].values[0]/(norm) # HTC of the next state\n",
    "    \n",
    "    new_state = [new_state_slice,new_value] # New state tuple\n",
    "    reward = (new_value.astype('float32') - baseline) # Reward from new state\n",
    "    \n",
    "    return new_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a61c4",
   "metadata": {},
   "source": [
    "## REINFORCE algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8dc83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: pre-allocating, loading and previous steps\n",
    "\n",
    "# Defining and loading the datasets\n",
    "\n",
    "vect = np.genfromtxt("../data/HTC/16layers_index.txt").astype(int) # Combination of materials\n",
    "htc_vals = np.genfromtxt("../data/HTC/16layers_index.txt", dtype = 'float64') # Associated HTC\n",
    "\n",
    "htc_series = pd.Series(data = htc_vals, index = tuple_or_list_to_str(vect)) # Series form, for much faster searching\n",
    "\n",
    "# Fix the random seed\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed) # Seed the random library\n",
    "torch.manual_seed(seed) # Seed the pytorch library\n",
    "np.random.seed(int(seed)) # Seed the numpy library\n",
    "\n",
    "# NN hyperparameters\n",
    "\n",
    "n_hidden = 4 # number of hidden layers\n",
    "n_neurons = 64 # number of neurons per layer\n",
    "lr = 0.00003 # NN learning rate\n",
    "\n",
    "# RL hyperparameters\n",
    "\n",
    "n_slice = 16 # Number of modifiable layers\n",
    "norm = 1e5 # Normalization constant for HTC, units (10^5 W/m^2 K)\n",
    "\n",
    "gamma = 0.99 # Discount rate\n",
    "\n",
    "n_steps = n_slice*2 # Length of each episode\n",
    "n_episode = 100000 # Number of episodes to train for\n",
    "\n",
    "# Initializations and Preallocations\n",
    "\n",
    "b = 0.0 # Baseline to subtract from the return\n",
    "\n",
    "base_seed = np.array([1,0,0,1,1,0,0,1,1,0,0,1,1,0,0,1]) # State to use as baseline for the rewards\n",
    "rew_base = htc_series[tuple_or_list_to_str([base_seed.astype(int)])][0]/(norm) # Baseline for the rewards\n",
    "\n",
    "start_seed = np.zeros((n_slice,)) # State to use as starting point for each episode\n",
    "start_state = [start_state_seed,htc_series[tuple_or_list_to_str([start_seed.astype(int)])][0]/(norm)] # Reset each episode\n",
    "   \n",
    "best_so_far = start_state[1] # Holds the best state found so far\n",
    "\n",
    "log_probs = [] # Will hold the log of the probabilities\n",
    "rewards = [] # Will hold the rewards for checking\n",
    "seen_so_far = [] # Will hold a list of all the states found in a training run\n",
    "\n",
    "last_state_record = [] # Will hold the last state of each episode\n",
    "best_state_record = [] # Will hold the best state found until up to episode\n",
    "loss_record = [] # Will hold the loss of each episode\n",
    "all_rewards_sum = [] # Will hold the sum of all rewards in each episode\n",
    "\n",
    "# Step 2: create the NN\n",
    "\n",
    "in_dim = n_slice # Number of neurons in the input layer\n",
    "out_dim = 2*n_slice # NUmber of neurons in the output layer\n",
    "\n",
    "layers = [nn.Linear(in_dim, n_neurons), nn.SELU()] # Create a list of all the layers, starting with input\n",
    "\n",
    "for i in range(n_hidden): # Include the hidden layers\n",
    "        \n",
    "    layers.append(nn.Linear(n_neurons,n_neurons))\n",
    "    layers.append(nn.SELU())\n",
    "    \n",
    "layers.append(nn.Linear(n_neurons,out_dim)) # Add the output layer\n",
    "    \n",
    "pi = nn.Sequential(*layers) # Create the final NN\n",
    "    \n",
    "opt = optim.Adam(pi.parameters(), lr) # Define the optimizer\n",
    "\n",
    "# Step 3: train the policy network\n",
    "\n",
    "for eps in range(n_episode):\n",
    "    \n",
    "    state = start_state # Reset the environment each episode\n",
    "    \n",
    "    for t in range(n_steps): \n",
    "        \n",
    "        # a) Choose the action given the state\n",
    "            \n",
    "        pdparam = pi.forward(torch.from_numpy(state[0].astype(np.float32))) # Forward pass, obtain output\n",
    "        pd = Categorical(logits = pdparam) # Obtain a probability distribution over the variables using the output\n",
    "        action = pd.sample() # Choose an action from the policy distribution\n",
    "        log_probss = pd.log_prob(action) # Obtain the log_prob of the chosen action\n",
    "        log_probs.append(log_probss) # Save the log_prob for the loss\n",
    "        \n",
    "        # b) Obtain the next state and reward from the action\n",
    "                    \n",
    "        state, reward = act_on_env7(action.item(), state, rew_base) # Response from environment\n",
    "        rewards.append(reward) # Save the reward for checking\n",
    "        \n",
    "        if state[1] not in seen_so_far: # Check if it's a new state\n",
    "            \n",
    "            seen_so_far.append(state[1]) # Save the state to the list\n",
    "            \n",
    "            if state[1] > best_so_far: # If it's better than any before, save it as the best\n",
    "                \n",
    "                best_so_far = state[1]\n",
    "                    \n",
    "    last_state_record.append(state[1]) # Save the last state HTC for monitoring\n",
    "    best_state_record.append(best_so_far) # Save the best state for monitoring\n",
    "    \n",
    "    total_reward = sum(rewards) # Sum of rewards of the episode\n",
    "    all_rewards_sum.append(total_reward) # Save the sum of all rewards for monitoring\n",
    "\n",
    "    # c) Obtain the loss\n",
    "        \n",
    "    returnss = np.empty(n_steps, dtype = np.float32) # Will hold all the returns\n",
    "    next_ret = 0.0 \n",
    "    \n",
    "    for t in reversed(range(n_steps)): # From last to first, we compute backwards the sum to get each return\n",
    "            \n",
    "        next_ret = rewards[t] + gamma*next_ret\n",
    "        returnss[t] = next_ret\n",
    "        \n",
    "    b = np.mean(returnss) # Baseline: average of the returns of the episode\n",
    "    returnss = returnss - b # Returns with baseline\n",
    "    returnss = torch.tensor(returnss) # Turn into a pytorch tensor\n",
    "    \n",
    "    log_props = torch.stack(log_probs) # Obtain the tensors from the list\n",
    "    \n",
    "    loss = -log_props*returnss # Calculate the gradient term, negative for maximization\n",
    "    \n",
    "    loss = torch.sum(loss) # Obtain the final loss\n",
    "    \n",
    "    loss_record.append(loss.item()) # Save the loss for monitoring\n",
    "    \n",
    "    opt.zero_grad() # Reset the gradients\n",
    "    loss.backward() # Backpropagation\n",
    "    opt.step() # Gradient descent (ascent)\n",
    "        \n",
    "    # clear memory for next episode\n",
    "        \n",
    "    log_probs = [] \n",
    "    rewards = [] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
